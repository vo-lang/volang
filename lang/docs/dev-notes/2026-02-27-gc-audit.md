# GC System Audit — 2026-02-27

Comprehensive review of the garbage collector: implementation, integration, diagnostics, and test coverage.

## Executive Summary

**The GC is fully implemented but never activated.** `gc.step()` and `gc.collect()` are never called from anywhere in the codebase. All GC-allocated objects accumulate forever until the VM process exits. The `Gc` struct has no `Drop` implementation, so even at shutdown, native resources (channels, maps, ports) are leaked.

Beyond the activation gap, there are root scanning omissions, write barrier coverage gaps, and the diagnostic/test infrastructure is insufficient to catch real GC bugs once collection is turned on.

---

## Files Reviewed

| File | Role |
|------|------|
| `vo-runtime/src/gc.rs` | GC core: alloc, mark, sweep, incremental state machine |
| `vo-runtime/src/gc_types.rs` | Object scanning by ValueKind, finalization |
| `vo-runtime/src/gc_debug.rs` | Debug/verification hooks (feature-gated) |
| `vo-runtime/src/jit_api.rs` | JIT ↔ GC interface: vo_gc_alloc, vo_gc_write_barrier, vo_gc_safepoint |
| `vo-runtime/src/ffi/mod.rs` | SentinelErrorCache (holds GcRef-containing interfaces) |
| `vo-runtime/src/objects/closure.rs` | Closure layout and capture access |
| `vo-vm/src/gc_roots.rs` | Root scanning: globals, fibers, stacks, defers, panic state, replay |
| `vo-vm/src/fiber.rs` | Fiber struct: stack, frames, defer, select, panic, closure_replay |
| `vo-vm/src/scheduler.rs` | Fiber lifecycle management |
| `vo-vm/src/vm/mod.rs` | VM main loop (run_fiber), scheduling loop |
| `vo-vm/src/vm/types.rs` | VmState (holds Gc instance), VmError |
| `vo-vm/src/exec/ptr.rs` | PtrSet write barrier |
| `vo-vm/src/exec/map.rs` | MapSet write barrier |
| `vo-vm/src/exec/slice.rs` | SliceNew, SliceSlice, SliceAppend |
| `vo-vm/src/exec/channel.rs` | ChanSend, ChanRecv |
| `vo-vm/src/exec/iface.rs` | IfaceAssign (ptr_clone) |
| `test_data/gc_*.vo` | 5 GC test files (4 root + 1 in dyn/) |
| `vo-runtime/src/jit_api.rs:vo_map_set` | JIT map set helper (no barrier) |

---

## 1. Implementation Issues

### 1.1 [P0] GC Collection Never Executes

**Finding:** `Gc::step()` and `Gc::collect()` are never called from any code path. Confirmed by grep across entire repo — zero call sites.

**Impact:** All GC-allocated objects live forever. Memory grows monotonically. Programs that run long (servers, GUI apps, benchmarks) will OOM.

**Current state of the GC state machine:**
- `Gc::alloc()` increments `debt` on every allocation
- `Gc::should_step()` checks `debt > 0` — never read
- `Gc::step()` implements full incremental mark-sweep — never called
- `Gc::collect()` implements full stop-the-world collection — never called
- `Gc::start_cycle()` flips white bits and scans roots — never called
- The entire Pause → Propagate → Atomic → Sweep pipeline is dead code

**Root cause:** GC integration was designed but the trigger point was never wired into the VM scheduling loop.

**Fix plan:**
1. Add GC step trigger in the scheduling loop (between timeslices, in `run_scheduling_loop`)
2. Wire `scan_roots` callback to `Vm::scan_roots()`
3. Wire `scan_object` callback to `gc_types::scan_object()`
4. Wire `finalize_object` callback to `gc_types::finalize_object()`
5. Check `gc.should_step()` after `schedule_next()` returns, before `run_fiber()`

```
// In run_scheduling_loop, after schedule_next():
if self.state.gc.should_step() {
    let module = self.module.as_ref().unwrap();
    let struct_metas = &module.struct_metas;
    self.state.gc.step(
        |gc| { /* scan_roots: scan globals + all fibers */ },
        |gc, obj| { gc_types::scan_object(gc, obj, struct_metas) },
        |obj| { gc_types::finalize_object(obj) },
    );
}
```

### 1.2 [P0] Dead Fibers Scanned as Roots — Object Retention Leak

**Finding:** `scan_fibers` iterates ALL entries in `scheduler.fibers` without checking `fiber.state`. `kill_current` marks the fiber as `Dead` and adds its slot to `free_slots`, but does NOT clear `stack`, `frames`, `defer_stack`, or any other fields. Until the slot is recycled by `reuse_or_spawn` (which calls `reset()`), the dead fiber's stack retains all GcRefs as live roots.

**Impact:** Programs that frequently spawn/kill goroutines accumulate a pool of dead fibers whose stacks keep entire object graphs alive indefinitely. This is a **memory retention leak** — objects that should be collected survive because dead fiber stacks still reference them.

**Fix plan:** Add state filter in `scan_fibers`:
```rust
for fiber in fibers {
    if fiber.state.is_dead() { continue; }
    // ... existing scanning code ...
}
```

Alternative: clear stack/frames in `kill_current`. But skipping dead fibers in scan is simpler and does not break the fiber reuse optimization (reuse still works via `reset()`).

### 1.3 [P0] closure_replay Scan is Undefined Behavior, Not "Conservative"

**Finding:** `scan_gcrefs` treats every non-zero `u64` in `closure_replay.results` as a GcRef and calls `gc.mark_gray()`, which dereferences the pointer to read its `GcHeader`. But `closure_replay.results` contains raw return slot values — these can be `int`, `bool`, `float64` bit patterns, or interface `slot0` metadata (packed itab_id|rttid|value_kind). Passing an arbitrary integer to `mark_gray` causes it to dereference a garbage address.

**Impact:** This is not a "false positive that only prevents collection". It is a **pointer dereference of arbitrary integers** → segfault or silent memory corruption. This will crash immediately once GC is activated if any extern closure callback returns non-GcRef values.

**Fix plan:** Upgrade `ClosureReplayState.results` from `Vec<Vec<u64>>` to `Vec<(Vec<u64>, Vec<SlotType>)>`. At the replay return site (when closure results are cached), the func_id is known — extract the return slot_types from `module.functions[func_id].slot_types[ret_start..ret_start+ret_slots]` and store alongside the values.

Then in `scan_fibers`, replace:
```rust
// WRONG: treats all values as GcRef
for vals in &fiber.closure_replay.results {
    scan_gcrefs(gc, vals);
}
```
With:
```rust
for (vals, slot_types) in &fiber.closure_replay.results {
    scan_slots_by_types(gc, vals, slot_types);
}
```

### 1.4 [P0] No Gc Drop — Memory Leak on Shutdown

**Finding:** `Gc` has no `impl Drop`. When `VmState` is dropped, all GC-managed heap objects leak. Native resources inside GC objects (Channel's `Box<QueueState>`, Map's `Box<HashMap>`, Port's `Arc<Mutex>`) are never finalized.

**Impact:** Every VM run leaks all heap memory. For embedded use (playground, studio, library), this means the host process accumulates leaked memory across runs.

**Fix plan:** `gc.rs` and `gc_types.rs` are both in the `vo-runtime` crate, so `Drop` can directly call `crate::gc_types::finalize_object`:

```rust
impl Drop for Gc {
    fn drop(&mut self) {
        for obj in self.all_objects.drain(..) {
            crate::gc_types::finalize_object(obj);
            let size = Self::object_size_bytes(obj);
            let raw = unsafe { (obj as *mut u8).sub(GcHeader::SIZE) };
            let layout = Layout::from_size_align(size, 8).unwrap();
            unsafe { heap_alloc::dealloc(raw, layout) };
        }
    }
}
```

No callback storage or design alternatives needed — same-crate access is sufficient.

### 1.5 [P0] SentinelErrorCache Holds GcRefs But Not Scanned

**Finding:** `SentinelErrorCache.inner` stores `Vec<(u64, u64)>` where each pair is an interface `(slot0, slot1)`. When `data_is_gc_ref(slot0)` is true, `slot1` is a GcRef. This cache is in `VmState` but is NOT included in `Vm::scan_roots()`.

**Impact:** If GC were running, sentinel error objects (e.g., `io.EOF`, `errors.ErrXxx`) would be collected while still referenced by the cache, causing use-after-free.

**Fix plan:** Add `scan_sentinel_errors(&mut gc, &sentinel_errors)` to `Vm::scan_roots()`:
```rust
fn scan_sentinel_errors(gc: &mut Gc, cache: &SentinelErrorCache) {
    for (_, errors) in cache.iter() {
        for &(slot0, slot1) in errors {
            if interface::data_is_gc_ref(slot0) && slot1 != 0 {
                gc.mark_gray(slot1 as GcRef);
            }
        }
    }
}
```

**Prerequisite:** `SentinelErrorCache` needs an `iter()` method or make `inner` pub(crate).

### 1.6 [P1] SelectState.registered_channels Not Scanned

**Finding:** `SelectState.registered_channels: Vec<GcRef>` holds channel GcRefs while a fiber is blocked in a select statement. `gc_roots.rs::scan_fibers` does NOT scan `fiber.select_state`.

**Impact:** If GC ran while a fiber is blocked in select, the registered channel objects could be collected. In most cases channels are also reachable from frame stack slots, but during select execution the original slot may be overwritten by other instructions, making `registered_channels` the only reference.

**Fix plan:** Add to `scan_fibers`:
```rust
if let Some(ref ss) = fiber.select_state {
    for &ch in &ss.registered_channels {
        if !ch.is_null() { gc.mark_gray(ch); }
    }
}
```

### 1.7 [P1] JIT panic_msg (InterfaceSlot) Not Scanned

**Finding:** `fiber.jit_panic_msg: InterfaceSlot` can hold a GcRef (when the panic value is a string or interface with heap data). It is set by JIT code via `vo_panic()` and read by the VM after JIT returns. `scan_fibers` does NOT scan this field.

**Impact:** Between JIT setting the panic and VM processing it, a GC could collect the panic value.

**Fix plan:** Add to `scan_fibers` (inside `#[cfg(feature = "jit")]`):
```rust
if fiber.jit_panic_flag {
    let val = fiber.jit_panic_msg;
    if val.is_ref_type() && val.slot1 != 0 {
        gc.mark_gray(val.as_ref());
    }
}
```

### 1.8 [P2] Generational GC Header Bits Unused

**Finding:** `GcHeader.marked` has age bits (G_YOUNG=0, G_SURVIVAL=1, G_OLD=2, G_TOUCHED=3) and methods `age()`, `set_age()`. These are never used. All objects are created with `G_YOUNG` and never promoted.

**Impact:** No functional impact. The header space is reserved for future generational GC but adds dead code.

**Status:** Informational. Keep for future use, no action needed now.

---

## 2. Integration Issues

### 2.1 [P0] No GC Trigger Point in VM Scheduling Loop

**Finding:** The VM main loop (`run_fiber`) executes up to `TIME_SLICE=1000` instructions per fiber, then yields. The scheduling loop (`run_scheduling_loop`) picks the next fiber and runs it. There is NO point where GC is checked or triggered.

**Design for integration:**

```
run_scheduling_loop:
    loop {
        schedule_next() → fiber_id
        
        // === GC CHECK POINT ===
        if gc.should_step() {
            gc.step(scan_roots, scan_object, finalize_object);
        }
        
        run_fiber(fiber_id)
        handle_exec_result(...)
    }
```

**Key constraint:** `scan_roots` needs `&mut Gc` but also needs to read `scheduler.fibers` and `globals`. The current `Vm::scan_roots(&mut self)` takes `&mut self` which works for this context since we're between fiber runs.

**Borrow splitting needed:** `gc.step()` takes `&mut Gc` via `scan_roots` callback, but `scan_roots` also needs `&self.scheduler.fibers` and `&self.state.globals`. Since `gc` is inside `self.state`, we need to split the borrow:

```rust
let gc = &mut self.state.gc as *mut Gc;
let gc_ref = unsafe { &mut *gc };
let globals = &self.state.globals;
let module = self.module.as_ref().unwrap();
let fibers = &self.scheduler.fibers;
let sentinel_errors = &self.state.sentinel_errors;

gc_ref.step(
    |gc| {
        scan_globals(gc, globals, &module.globals);
        scan_fibers(gc, fibers, &module.functions);
        scan_sentinel_errors(gc, sentinel_errors);
    },
    |gc, obj| gc_types::scan_object(gc, obj, &module.struct_metas),
    |obj| gc_types::finalize_object(obj),
);
```

### 2.2 [P1] Write Barrier Coverage Gaps

Write barriers are needed when storing a GcRef into a **heap object** that might already be BLACK. Stack writes do NOT need barriers because the stack is always scanned as a root.

**Current coverage:**

| Operation | Has Barrier | Notes |
|-----------|-------------|-------|
| PtrSet (flags bit0) | ✅ | Correct: codegen sets flag for GcRef fields |
| MapSet (flags bit0/1) | ⚠️ | Has barrier but mixed-slot UB — see 2.3 |
| SliceSet | ❌ | Writes into backing array (heap). Missing barrier for GcRef elements |
| SliceAppend | ❌ | May write into existing backing array. Missing barrier |
| ArraySet | ❌ | Writes into heap array. Missing barrier for GcRef elements |
| ChanSend (VM) | ❌ | Writes values into QueueState buffer (accessible from GC object) |
| ChanSend (JIT) | ❌ | Same gap via callback path |
| ClosureNew captures | ✅ (implicit) | New object gets current_white, so no barrier needed |
| IfaceAssign ptr_clone | ✅ (implicit) | New clone gets current_white |

**Fix plan for SliceSet/ArraySet:** For simple GcRef elements (pointer, slice, map, string, closure, channel), codegen knows the element is a single GcRef — emit a simple `gc.write_barrier(backing_array, val as GcRef)`. For struct elements with mixed slots, need a typed barrier helper (see 2.3 for the unified approach).

**Fix plan for SliceAppend:** If `slice::append` reuses the backing array (no growth), need barrier. If it allocates new array, the new array gets current_white so no barrier needed. The `slice::append` function in `vo-runtime/src/objects/slice.rs` should call `gc.write_barrier(backing_array, val)` when reusing existing capacity.

**Fix plan for ChanSend:** Channel buffer is inside a `Box<QueueState>` within the GC object. Values written to the buffer are accessed during `scan_channel`. Need typed barrier based on element's slot_types.

**Note:** These barriers are currently harmless to add even though GC isn't running — `write_barrier()` checks `state != Propagate` and returns early.

### 2.3 [P0] MapSet Barrier is Unsound + JIT Path Missing Entirely

**Finding (VM):** The current VM MapSet barrier blindly iterates all key/value slots and calls `gc.write_barrier(m, v as GcRef)` for any non-zero value. For mixed-slot types (e.g., a struct with int and pointer fields), non-zero integer values are cast to `GcRef` and passed to `write_barrier`, which calls `Self::header(child)` — **dereferencing an arbitrary integer as a pointer**. This is UB.

**Finding (JIT):** `vo_map_set` in `jit_api.rs` calls `map::set()` with NO barrier at all. JIT MapSet goes through this helper, completely bypassing the VM barrier path.

**Impact:** Once GC is activated in Propagate state, every MapSet with non-GcRef value slots will dereference garbage. The JIT path will silently break tri-color invariants.

**Fix plan:** Implement a **typed barrier helper** shared by VM and JIT:
```rust
fn typed_write_barrier(
    gc: &mut Gc,
    parent: GcRef,
    vals: &[u64],
    slot_types: &[SlotType],
) {
    for (i, &st) in slot_types.iter().enumerate() {
        match st {
            SlotType::GcRef => {
                if vals[i] != 0 {
                    gc.write_barrier(parent, vals[i] as GcRef);
                }
            }
            SlotType::Interface0 => {
                if i + 1 < vals.len()
                    && interface::data_is_gc_ref(vals[i])
                    && vals[i + 1] != 0
                {
                    gc.write_barrier(parent, vals[i + 1] as GcRef);
                }
            }
            _ => {}
        }
    }
}
```

For MapSet: codegen must pass key/val slot_types (or at minimum, a flag per slot indicating GcRef). The current `flags bit0/1` is too coarse — it says "key/val MAY contain GcRef" but doesn't say WHICH slots.

For `vo_map_set` (JIT): add barrier call after `map::set()`, using the same typed helper.

### 2.4 [P1] JIT vo_gc_alloc Does Not Trigger GC

**Finding:** `vo_gc_alloc` calls `gc.alloc()` which increments `debt` but never checks `should_step()`. The JIT safepoint comment says "GC triggers only at call boundaries" including `vo_gc_alloc`, but this is incorrect — alloc just allocates.

**Fix plan:** After allocation in `vo_gc_alloc`, check and trigger GC:
```rust
pub extern "C" fn vo_gc_alloc(gc: *mut Gc, meta: u32, slots: u32) -> u64 {
    let gc = unsafe { &mut *gc };
    let value_meta = ValueMeta::from_raw(meta);
    let result = gc.alloc(value_meta, slots as u16) as u64;
    // GC step will be triggered at next scheduling boundary
    // (cannot trigger here — no access to roots/module)
    result
}
```

Actually, triggering GC inside `vo_gc_alloc` is problematic because the JIT caller may hold un-rooted GcRefs in SSA variables. The correct approach is:
1. `alloc()` accumulates debt
2. GC runs at scheduling boundaries (between timeslices)
3. For JIT: GC runs when JIT returns to VM (function call boundaries)

This is already the intended design per the safepoint comment. The fix is just to wire up the scheduling-boundary trigger (issue 2.1).

### 2.5 [P2] JIT Safepoint Flag Always False

**Finding:** `fiber.jit_safepoint_flag` is always `false`. `vo_gc_safepoint` is a no-op. JIT code never checks the safepoint flag in loop back-edges.

**Impact:** Long-running JIT loops without function calls will delay GC. This is a latency issue, not a correctness issue, because JIT's non-moving GC design means GcRefs in SSA variables remain valid.

**Fix plan (deferred):** When GC latency becomes a problem, add safepoint polls in JIT loop back-edges that check the flag and yield to the scheduling loop.

---

## 3. Diagnostics Issues

### 3.1 [P1] gc_debug Module is Disconnected

**Finding:** The `gc_debug` module provides hooks (`on_alloc`, `on_free`, `on_barrier`, `on_ptr_write`, `verify_tri_color`) but:
- `on_alloc` and `on_free` are called from `gc.rs` (good)
- `on_barrier` is called from `gc.rs::write_barrier` (good)
- `on_ptr_write` is NEVER called — no write tracking
- `verify_tri_color` is NEVER called — no automated invariant checking
- Uses `thread_local!` — each island thread gets independent state, cannot provide global GC view

**Fix plan:**
1. Call `on_ptr_write` from all heap write sites (PtrSet, MapSet, SliceSet, ArraySet, ChanSend)
2. Call `verify_tri_color` at end of atomic phase in `gc.step()`
3. Keep thread_local (acceptable since each island has its own GC), but document that cross-island aggregate stats require explicit merging

### 3.2 [P2] No GC Statistics or Tuning API

**Finding:** `Gc` has `total_bytes()` and `object_count()` but no way to:
- Query cycle count, time spent in GC, pause times
- Tune parameters (pause, stepmul, stepsize) at runtime
- Access GC stats from Vo code (e.g., `runtime.GCStats()`)

**Fix plan (deferred):** Add `GcStats` struct and `Gc::stats()` method. Add extern functions to expose stats to Vo code.

### 3.3 [P2] No GC Stress Testing Mode

**Finding:** No mode to force GC on every allocation (or every N allocations) for finding barrier bugs and root scanning gaps.

**Fix plan:** Add `gc-stress` feature that calls `collect()` on every `alloc()`:
```rust
fn alloc_inner(...) -> GcRef {
    #[cfg(feature = "gc-stress")]
    if self.all_objects.len() > 0 {
        // caller must pass scan_roots + scan_object + finalize_object
        // This is architecturally hard — see design note below
    }
    // normal alloc...
}
```

**Design challenge:** `alloc()` doesn't have access to root scanning callbacks. Options:
- (a) Store callbacks in Gc (adds lifetime complexity)
- (b) Stress mode is VM-level: check flag in scheduling loop, collect every iteration
- (c) Separate `gc_stress_check()` function called from VM after every alloc-heavy instruction

Recommend **(b)** — simplest and catches all bugs that matter.

---

## 4. Test Case Issues

### 4.1 [P0] Existing GC Tests Don't Test GC

**Finding:** The 5 existing test files are:
- `gc_closure_capture.vo` — Tests closure captures work. No GC pressure.
- `gc_map_stress.vo` — Creates ~300 map entries. No GC trigger.
- `gc_slice_grow.vo` — Tests slice operations. No GC trigger.
- `gc_struct_nested.vo` — Tests nested struct pointers. No GC trigger.
- `dyn/gc_interface_dynamic.vo` — Tests dynamic access with interfaces. No GC trigger.

These test object allocation and usage correctness, NOT garbage collection. Since GC never runs, they would pass even if the GC were completely broken.

**Fix plan:** After wiring up GC (issues 1.1 + 2.1), create real GC tests:

### 4.2 Required Test Cases

#### Phase 1: Basic Collection (after 1.1 + 2.1)

**T1: gc_basic_collect.vo** — Allocate objects, drop references, force GC, verify memory decreases.
```go
// Allocate many objects, let them go out of scope
for i := 0; i < 10000; i++ {
    _ = make([]int, 100) // each ~800 bytes, total ~8MB
}
// After GC: memory should be near baseline
// (Need runtime.GCStats() extern to verify, or just verify no crash)
```

**T2: gc_live_objects_survive.vo** — Verify live objects are NOT collected.
```go
held := make([]*Node, 1000)
for i := 0; i < 1000; i++ {
    held[i] = &Node{Value: i}
}
// Force GC (allocate enough to trigger)
for i := 0; i < 10000; i++ {
    _ = make([]int, 100)
}
// Verify all held objects are intact
for i := 0; i < 1000; i++ {
    assert(held[i].Value == i)
}
```

**T3: gc_nested_refs.vo** — Verify transitive references keep objects alive.
```go
root := &Node{Value: 1}
root.Left = &Node{Value: 2}
root.Left.Right = &Node{Value: 3}
// Force GC
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
// Verify chain intact
assert(root.Left.Right.Value == 3)
```

#### Phase 2: Root Scanning (after 1.3 + 1.4 + 1.5)

**T4: gc_global_roots.vo** — Global variables survive GC.
```go
var globalSlice = []int{1, 2, 3}
var globalMap = map[string]int{"a": 1}
func main() {
    // Force GC
    for i := 0; i < 10000; i++ { _ = make([]int, 100) }
    assert(globalSlice[0] == 1)
    assert(globalMap["a"] == 1)
}
```

**T5: gc_defer_roots.vo** — Objects in defer stack survive GC.
```go
func foo() {
    s := []int{1, 2, 3}
    defer func() { assert(s[0] == 1) }()
    // Force GC
    for i := 0; i < 10000; i++ { _ = make([]int, 100) }
}
```

**T6: gc_closure_capture_survive.vo** — Captured variables survive GC.
```go
func make_adder(x int) func(int) int {
    data := []int{x} // heap-escaped
    return func(y int) int { return data[0] + y }
}
add5 := make_adder(5)
// Force GC
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
assert(add5(3) == 8)
```

**T7: gc_goroutine_roots.vo** — Objects in blocked goroutine stacks survive GC.
```go
ch := make(chan int)
go func() {
    data := []int{42}
    ch <- data[0] // blocks until receiver ready
}()
// Force GC while goroutine is blocked
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
val := <-ch
assert(val == 42)
```

#### Phase 3: Write Barriers (after 2.2)

**T8: gc_slice_set_barrier.vo** — SliceSet with GcRef elements during incremental GC.
```go
type Box struct { Value int }
s := make([]*Box, 100)
for i := 0; i < 100; i++ {
    s[i] = &Box{Value: i}
}
// Trigger incremental GC, then mutate slice elements
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
s[50] = &Box{Value: 999}
// More GC pressure
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
assert(s[50].Value == 999)
```

**T9: gc_map_barrier.vo** — MapSet during incremental GC.
```go
m := make(map[int]*Box)
for i := 0; i < 100; i++ {
    m[i] = &Box{Value: i}
}
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
m[50] = &Box{Value: 999}
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
assert(m[50].Value == 999)
```

#### Phase 4: Finalization (after 1.2)

**T10: gc_finalize_channel.vo** — Channel native resources freed after collection.
```go
for i := 0; i < 1000; i++ {
    ch := make(chan int, 10)
    _ = ch // channel goes out of scope
}
// Force GC — should not crash/leak
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
```

**T11: gc_finalize_map.vo** — Map native resources freed after collection.
```go
for i := 0; i < 1000; i++ {
    m := make(map[int]int)
    for j := 0; j < 100; j++ { m[j] = j }
}
for i := 0; i < 10000; i++ { _ = make([]int, 100) }
```

---

## 5. Implementation Plan

### Phase 1: Foundation (must-do before enabling GC)

| # | Task | Files | Priority |
|---|------|-------|----------|
| 1 | Skip dead fibers in `scan_fibers` | `gc_roots.rs` | P0 |
| 2 | Upgrade `closure_replay.results` to carry SlotTypes; replace `scan_gcrefs` with `scan_slots_by_types` | `fiber.rs`, `gc_roots.rs`, `exec/unwind.rs`, `exec/extern_call.rs` | P0 |
| 3 | Implement `Drop for Gc` (direct call to `crate::gc_types::finalize_object`) | `gc.rs` | P0 |
| 4 | Add `scan_sentinel_errors` to `scan_roots` | `gc_roots.rs`, `ffi/mod.rs` | P0 |
| 5 | Add `scan_select_state` to `scan_fibers` | `gc_roots.rs` | P1 |
| 6 | Add `scan_jit_panic_msg` to `scan_fibers` | `gc_roots.rs` | P1 |

### Phase 2: Activation

| # | Task | Files | Priority |
|---|------|-------|----------|
| 7 | Wire GC step into scheduling loop | `vm/mod.rs` | P0 |
| 8 | Handle borrow splitting (gc vs scheduler/globals) | `vm/mod.rs` | P0 |
| 9 | Add GC stress mode (collect every scheduling iteration) | `vm/mod.rs` | P2 |

### Phase 3: Write Barriers

| # | Task | Files | Priority |
|---|------|-------|----------|
| 10 | Implement `typed_write_barrier` helper | `gc.rs` or `gc_types.rs` | P0 |
| 11 | Fix MapSet barrier: use typed helper, fix JIT `vo_map_set` | `exec/map.rs`, `jit_api.rs` | P0 |
| 12 | Add barrier flag to SliceSet instruction | `codegen`, `exec/slice.rs`, `vm/mod.rs` | P1 |
| 13 | Add barrier flag to ArraySet instruction | `codegen`, `vm/mod.rs` | P1 |
| 14 | Add barrier in SliceAppend (reuse path) | `objects/slice.rs` | P1 |
| 15 | Add barrier for ChanSend (GcRef elements) | `exec/channel.rs` | P1 |
| 16 | Mirror all VM barriers in JIT code generation | `vo-jit/src/` | P1 |

### Phase 4: Diagnostics

| # | Task | Files | Priority |
|---|------|-------|----------|
| 17 | Wire `verify_tri_color` into atomic phase | `gc.rs` | P1 |
| 18 | Wire `on_ptr_write` into all heap write sites | Various exec files | P1 |
| 19 | Add GcStats struct and Gc::stats() | `gc.rs` | P2 |

### Phase 5: Tests

| # | Task | Priority |
|---|------|----------|
| 20 | T1-T3: Basic collection tests | P0 |
| 21 | T4-T7: Root scanning tests | P1 |
| 22 | T8-T9: Write barrier tests | P1 |
| 23 | T10-T11: Finalization tests | P1 |

### Execution Order

```
Phase 1 (root scanning + Drop) → Phase 2 (activation) → Phase 5.T1-T3 (verify basics)
→ Phase 3 (barriers) → Phase 5.T8-T9 (verify barriers)
→ Phase 4 (diagnostics) → Phase 5.T4-T7,T10-T11 (full test suite)
```

**Estimated scope:** ~1000-1500 lines of code changes across ~18 files.

---

## 6. Technical Decisions

### Decision 1: GC Trigger Strategy

**Chosen:** Step only in `run_scheduling_loop`, after `schedule_next()` and before `run_fiber()`.

**Rationale:** GC step is incremental — each invocation processes at most `stepsize * stepmul / 100` bytes. If all fibers are blocked and no new allocations occur, `debt` stays flat and `should_step()` returns false. If allocations happen (e.g., I/O callback), fibers become runnable and enter the scheduling loop naturally. Idle-time GC can be added later if retention latency proves problematic.

### Decision 2: closure_replay Root Representation

**Chosen:** Upgrade `ClosureReplayState.results` from `Vec<Vec<u64>>` to `Vec<(Vec<u64>, Vec<SlotType>)>`.

**Rationale:** The current `scan_gcrefs` approach is UB (dereferences arbitrary integers). Carrying slot_types per result is the only correct fix. At the replay push site, `func_id` is known, so extracting `slot_types[ret_start..ret_start+ret_slots]` from `module.functions` is straightforward. Storage overhead is minimal (one `Vec<SlotType>` per result, results are short-lived).

### Decision 3: Write Barrier Unification

**Chosen:** VM and JIT share a single `typed_write_barrier(gc, parent, vals, slot_types)` helper.

**Rationale:** Maintaining two barrier implementations is a correctness hazard. The helper lives in `vo-runtime` (accessible by both VM and JIT callbacks). For simple single-GcRef writes (pointer, slice, map, string elements), codegen can emit a direct `gc.write_barrier(parent, val)` without the typed helper. The typed helper is only needed for struct/interface elements with mixed slots.

---

## 7. Design Notes

### Why the backward barrier is correct

The current `write_barrier` implements a **backward barrier** (Dijkstra-style): when a black parent writes a white child, the parent is set back to white and added to `grayagain`. This is correct for the incremental mark-sweep design:
- During Propagate: barrier ensures modified objects are re-scanned
- During Atomic: grayagain is fully processed
- During Sweep/Pause: barrier is a no-op (checked via `state != Propagate`)

### Why stack writes don't need barriers

The fiber stack is always scanned as a root at the start of each GC cycle (`scan_fibers`). Stack-local GcRefs are always reachable. Only heap-to-heap writes need barriers.

### Why JIT safepoints can be deferred

The non-moving GC means GcRef pointer values never change. JIT code holding GcRefs in SSA registers is safe even if GC runs concurrently. The only risk is latency: long JIT loops delay GC. This is acceptable for now.

### Closure capture scanning correctness

All closure captures are GcRefs to heap-escaped variables. The scan treats all captures as GcRefs, which is correct. If a capture slot somehow contained a non-GcRef value, it would be treated as a pointer — but this cannot happen by construction (codegen only stores escaped variable GcRefs as captures).

### Why closure_replay scan is NOT "conservative safe" (corrected)

The original audit incorrectly stated that scanning closure_replay results with `scan_gcrefs` is "conservative but safe". This is wrong: `mark_gray` dereferences the pointer to read `GcHeader`, so passing a non-pointer value (int, float, interface slot0 metadata) causes UB. The fix is to carry `SlotType` information alongside cached results (see Decision 2 above).

---

## Appendix: Revision History

- **2026-02-27 v1:** Initial audit.
- **2026-02-27 v2:** Incorporated audit-of-audit corrections:
  - Added P0: dead fiber root scanning leak (1.2)
  - Upgraded closure_replay scan from "conservative safe" to UB/P0 (1.3)
  - Promoted SentinelErrorCache to P0 (1.5)
  - Added P0: MapSet barrier UB + JIT path missing (2.3)
  - Fixed Gc Drop discussion (same crate, direct call)
  - Added Technical Decisions section (trigger strategy, replay types, barrier unification)
  - Fixed test file count (5 not 4)
  - Fixed gc_debug thread_local characterization
  - Added SelectState risk level nuance
- **2026-02-27 v3:** Implementation results:
  - **Completed (12 files modified):**
    - `gc_roots.rs`: Skip dead fibers, scan sentinel errors, scan select_state, scan JIT panic_msg, type-safe closure_replay scanning, gc_step() with borrow splitting
    - `fiber.rs`: closure_replay.results upgraded to `Vec<(Vec<u64>, Vec<SlotType>)>`
    - `exec/unwind.rs`: Push slot_types alongside closure replay return values
    - `gc.rs`: mark_gray/write_barrier alignment guards
    - `gc_types.rs`: `typed_write_barrier()` and `typed_write_barrier_by_meta()` helpers
    - `exec/map.rs`: Type-safe MapSet barrier via `barrier_map_slots()`
    - `jit_api.rs`: JIT vo_map_set barrier via `typed_write_barrier_by_meta()`
    - `objects/slice.rs`: SliceAppend reuse-path barrier
    - `exec/channel.rs` + `vm/jit/callbacks/channel.rs`: ChanSend barrier (VM + JIT)
    - `ffi/mod.rs`: `SentinelErrorCache::iter_values()` for GC scanning
    - `vm/mod.rs`: gc_step() call site (currently disabled)
  - **Blocked on codegen slot_types bug (NEW P0 FINDING):**
    - When gc_step() is enabled and runs root scanning, `scan_slots_by_types` encounters non-GcRef values (small integers like `0x2`, negative numbers like `-6`) in slots typed as `SlotType::GcRef`. `mark_gray` dereferences these → segfault.
    - This is a **pre-existing codegen bug** in `vo-codegen` slot_types generation. It was invisible because GC never ran before.
    - gc_step() and Gc::Drop are both disabled until this is resolved.
    - Several pre-existing release-mode segfaults confirmed (`print_format.vo`, `gc_map_stress.vo` — crash on original code too).
  - **Next steps:** Audit `vo-codegen` slot_types generation to find and fix mistyped slots. Once fixed, uncomment gc_step() and add Gc::Drop.
